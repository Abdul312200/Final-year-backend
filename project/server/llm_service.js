/**
 * LLM Service â€” Gemini Primary + Ollama Fallback
 * Primary:  Google Gemini 1.5 Flash (free tier)
 * Fallback: Ollama local LLM (llama3.2 / mistral)
 *
 * âš ï¸  Set GEMINI_API_KEY in .env with a fresh key (never commit raw keys).
 */

import { GoogleGenerativeAI, HarmCategory, HarmBlockThreshold } from '@google/generative-ai';
import ollama from 'ollama';
import axios from 'axios';

// â”€â”€â”€ Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const GEMINI_API_KEY = process.env.GEMINI_API_KEY || '';
const GEMINI_MODEL   = process.env.GEMINI_MODEL   || 'gemini-1.5-flash';
const OLLAMA_HOST    = process.env.OLLAMA_HOST    || 'http://127.0.0.1:11434';
const OLLAMA_MODEL   = process.env.OLLAMA_MODEL   || 'llama3.2';

// â”€â”€â”€ System Prompts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const SYSTEM_PROMPT_EN = `You are FinTechIQ â€” an expert stock market and financial AI assistant.

STRICT RULES you MUST follow:
1. ONLY answer questions about stocks, shares, market analysis, trading, investments, finance, mutual funds, ETFs, commodities (gold/silver/oil), and financial markets.
2. If asked about unrelated topics (weather, sports, movies, cooking, health advice, politics, relationships, etc.) â€” politely DECLINE and redirect to stock topics.
3. Support English, Tamil, and Tanglish (Tamil written in English letters). When user writes in Tamil or Tanglish, respond in Tamil.
4. Keep answers CONCISE, structured with bullet points and emojis.
5. For buy/sell advice always add: âš ï¸ "This is not financial advice. Consult a SEBI-registered advisor."
6. Reference FinTechIQ tools when relevant:
   - ğŸ“ˆ Stock prediction: "predict AAPL"
   - ğŸ“Š Analysis: "analyze TCS.NS"
   - âš–ï¸ Compare: "compare AAPL vs TSLA"
   - ğŸ’° Price: "price MSFT"
7. Use Indian context (â‚¹, NSE/BSE) when applicable.
8. Never make up stock prices â€” say "use our live price tool."`;

const SYSTEM_PROMPT_TA = `à®¨à¯€à®™à¯à®•à®³à¯ FinTechIQ â€” à®’à®°à¯ à®¨à®¿à®ªà¯à®£ à®ªà®™à¯à®•à¯ à®šà®¨à¯à®¤à¯ˆ AI à®‰à®¤à®µà®¿à®¯à®¾à®³à®°à¯.

à®•à®Ÿà¯à®Ÿà®¾à®¯ à®µà®¿à®¤à®¿à®•à®³à¯:
1. à®ªà®™à¯à®•à¯à®•à®³à¯, à®šà®¨à¯à®¤à¯ˆ, à®®à¯à®¤à®²à¯€à®Ÿà¯, à®µà®°à¯à®¤à¯à®¤à®•à®®à¯, à®¨à®¿à®¤à®¿ à®¤à¯Šà®Ÿà®°à¯à®ªà®¾à®© à®•à¯‡à®³à¯à®µà®¿à®•à®³à¯à®•à¯à®•à¯ à®®à®Ÿà¯à®Ÿà¯à®®à¯‡ à®ªà®¤à®¿à®²à¯ à®šà¯Šà®²à¯à®²à¯à®™à¯à®•à®³à¯.
2. à®¤à¯Šà®Ÿà®°à¯à®ªà®¿à®²à¯à®²à®¾à®¤ à®•à¯‡à®³à¯à®µà®¿à®•à®³à¯ˆ à®®à®±à¯à®•à¯à®•à®µà¯à®®à¯, à®ªà®™à¯à®•à¯ à®•à¯‡à®³à¯à®µà®¿à®•à®³à¯à®•à¯à®•à¯ à®¤à®¿à®°à¯à®ªà¯à®ªà®¿ à®µà®¿à®Ÿà®µà¯à®®à¯.
3. à®¤à®®à®¿à®´à¯, à®†à®™à¯à®•à®¿à®²à®®à¯, Tanglish à®†à®¤à®°à®¿à®•à¯à®•à®µà¯à®®à¯. à®¤à®®à®¿à®´à®¿à®²à¯ à®•à¯‡à®Ÿà¯à®•à¯à®®à¯à®ªà¯‹à®¤à¯ à®¤à®®à®¿à®´à®¿à®²à¯ à®ªà®¤à®¿à®²à¯ à®šà¯Šà®²à¯à®²à¯à®™à¯à®•à®³à¯.
4. à®šà¯à®°à¯à®•à¯à®•à®®à®¾à®©, bullet points à®®à®±à¯à®±à¯à®®à¯ emoji à®‰à®Ÿà®©à¯ à®ªà®¤à®¿à®²à¯ à®šà¯Šà®²à¯à®²à¯à®™à¯à®•à®³à¯.
5. âš ï¸ "à®‡à®¤à¯ à®¨à®¿à®¤à®¿ à®†à®²à¯‹à®šà®©à¯ˆ à®…à®²à¯à®². SEBI à®ªà®¤à®¿à®µà¯à®šà¯†à®¯à¯à®¯à®ªà¯à®ªà®Ÿà¯à®Ÿ à®†à®²à¯‹à®šà®•à®°à¯ˆ à®…à®£à¯à®•à®µà¯à®®à¯."`;

// â”€â”€â”€ Gemini Client â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
let geminiClient = null;
let geminiReady  = false;

function initGemini() {
  if (!GEMINI_API_KEY || GEMINI_API_KEY === 'YOUR_NEW_GEMINI_KEY_HERE') {
    console.warn('âš ï¸  GEMINI_API_KEY not set â€” Gemini LLM disabled. Add it to .env');
    return false;
  }
  try {
    geminiClient = new GoogleGenerativeAI(GEMINI_API_KEY);
    geminiReady  = true;
    console.log(`âœ… Gemini LLM ready (${GEMINI_MODEL})`);
    return true;
  } catch (err) {
    console.error('âŒ Gemini init error:', err.message);
    return false;
  }
}

// Convert OpenAI-style history â†’ Gemini format
function toGeminiHistory(messages) {
  return messages.map(m => ({
    role:  m.role === 'assistant' ? 'model' : 'user',
    parts: [{ text: m.content }],
  }));
}

async function askGemini({ userMessage, conversationHistory = [], lang = 'en' }) {
  if (!geminiReady) {
    if (!initGemini()) return { success: false, reason: 'gemini_not_configured', text: null };
  }
  try {
    const sysInstruction = lang === 'ta' ? SYSTEM_PROMPT_TA : SYSTEM_PROMPT_EN;
    const model = geminiClient.getGenerativeModel({
      model: GEMINI_MODEL,
      systemInstruction: sysInstruction,
      safetySettings: [
        { category: HarmCategory.HARM_CATEGORY_HARASSMENT,        threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE },
        { category: HarmCategory.HARM_CATEGORY_HATE_SPEECH,       threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE },
        { category: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT, threshold: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE },
        { category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE },
      ],
      generationConfig: { temperature: 0.4, topP: 0.9, maxOutputTokens: 600 },
    });
    const history = toGeminiHistory(conversationHistory.slice(-6));
    const chat    = model.startChat({ history });
    const result  = await chat.sendMessage(userMessage);
    const text    = result.response.text();
    return { success: true, text, model: GEMINI_MODEL, source: 'gemini' };
  } catch (err) {
    const msg = err?.message || String(err);
    if (msg.includes('quota') || msg.includes('429')) {
      console.warn('âš ï¸  Gemini quota exceeded â€” falling back to Ollama');
      return { success: false, reason: 'quota_exceeded', text: null };
    }
    console.error('Gemini error:', msg);
    return { success: false, reason: msg, text: null };
  }
}

// â”€â”€â”€ Ollama â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
let ollamaAvailable = null;
let ollamaLastCheck = 0;

async function isOllamaAvailable() {
  const now = Date.now();
  if (ollamaAvailable !== null && now - ollamaLastCheck < 60000) return ollamaAvailable;
  try {
    const res = await axios.get(`${OLLAMA_HOST}/api/tags`, { timeout: 3000 });
    ollamaAvailable = res.status === 200;
  } catch { ollamaAvailable = false; }
  ollamaLastCheck = Date.now();
  return ollamaAvailable;
}

async function listOllamaModels() {
  try {
    const res = await axios.get(`${OLLAMA_HOST}/api/tags`, { timeout: 5000 });
    return (res.data.models || []).map(m => m.name);
  } catch { return []; }
}

async function getBestOllamaModel() {
  const models   = await listOllamaModels();
  if (!models.length) return null;
  const priority = ['llama3.2', 'llama3.1', 'llama3', 'mistral', 'gemma2', 'gemma', 'phi3', 'phi'];
  for (const p of priority) {
    const found = models.find(m => m.toLowerCase().startsWith(p));
    if (found) return found;
  }
  return models[0];
}

async function askOllama({ userMessage, conversationHistory = [], lang = 'en' }) {
  const available = await isOllamaAvailable();
  if (!available) return { success: false, reason: 'ollama_not_running', text: null };
  const model = await getBestOllamaModel();
  if (!model)  return { success: false, reason: 'no_ollama_model', text: null };
  const systemPrompt = lang === 'ta' ? SYSTEM_PROMPT_TA : SYSTEM_PROMPT_EN;
  const messages = [
    { role: 'system', content: systemPrompt },
    ...conversationHistory.slice(-5),
    { role: 'user', content: userMessage },
  ];
  try {
    const response = await ollama.chat({
      model, messages,
      options: { temperature: 0.4, top_p: 0.9, num_predict: 500 },
      stream: false,
    });
    return { success: true, text: response.message?.content || '', model, source: 'ollama' };
  } catch (err) {
    console.error('Ollama error:', err.message);
    return { success: false, reason: err.message, text: null };
  }
}

async function ensureOllamaModel(modelName = OLLAMA_MODEL) {
  const models = await listOllamaModels();
  if (models.some(m => m.startsWith(modelName))) return true;
  console.log(`ğŸ“¥ Pulling Ollama model: ${modelName}...`);
  try {
    const stream = await ollama.pull({ model: modelName, stream: true });
    for await (const chunk of stream) { if (chunk.status) process.stdout.write('.'); }
    console.log(`\nâœ… ${modelName} ready`);
    return true;
  } catch (err) {
    console.error(`âŒ Pull failed: ${err.message}`);
    return false;
  }
}

// â”€â”€â”€ Main Entry â€” Gemini first, Ollama fallback â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async function askLLM({ userMessage, conversationHistory = [], lang = 'en' }) {
  const geminiResult = await askGemini({ userMessage, conversationHistory, lang });
  if (geminiResult.success) return geminiResult;
  console.log(`Gemini failed (${geminiResult.reason}), trying Ollama...`);
  const ollamaResult = await askOllama({ userMessage, conversationHistory, lang });
  if (ollamaResult.success) return ollamaResult;
  return { success: false, reason: 'all_llm_failed', text: null };
}

async function getLLMStatus() {
  const geminiOk   = !!(GEMINI_API_KEY && GEMINI_API_KEY !== 'YOUR_NEW_GEMINI_KEY_HERE');
  const ollamaOk   = await isOllamaAvailable();
  const models     = ollamaOk ? await listOllamaModels() : [];
  const bestOllama = ollamaOk ? await getBestOllamaModel() : null;
  return {
    gemini: { available: geminiOk, model: GEMINI_MODEL, status: geminiOk ? 'âœ… Configured' : 'âš ï¸  Key not set â€” add GEMINI_API_KEY to .env' },
    ollama: { available: ollamaOk, models, activeModel: bestOllama, status: ollamaOk ? `âœ… Running (${models.length} model(s))` : 'âš ï¸  Not running â€” run: ollama serve', installGuide: 'https://ollama.ai' },
    primary: geminiOk ? 'gemini' : ollamaOk ? 'ollama' : 'none',
  };
}

// Initialise on import
initGemini();

export {
  askLLM,
  askGemini,
  askOllama,
  isOllamaAvailable,
  listOllamaModels,
  getBestOllamaModel,
  ensureOllamaModel,
  getLLMStatus,
};
